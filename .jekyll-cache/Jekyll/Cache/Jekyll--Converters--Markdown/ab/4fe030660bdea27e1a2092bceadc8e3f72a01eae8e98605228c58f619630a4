I"≈<p><img src="assets/img/banner.jpg" alt="" /></p>

<p>Many exciting computer vision applications operate at the edge; this could be in drones, IoT devices, or AR/VR systems. These demand visual processing that is fast, energy-efficient, and privacy-preserving. On-sensor vision meets these needs by unifying sensing and computation within a single chip, producing information-rich outputs instead of raw images. By minimizing costly data transfers, such architectures enable real-time, low-power operation. Prototypes like Manchester‚Äôs SCAMP integrate sensing, memory, and parallel processing directly on-chip. This workshop explores algorithms and architectures for near-pixel, in-sensor, and stacked sensor-processor devices, and how to best partition computation between on-sensor and off-sensor processing.</p>

<h3 id="about-this-workshop">About this Workshop</h3>
<p>While much focus in the recent explosion of computer vision has been on cloud-based processing of datasets, some of the most challenging and exciting applications are at the edge, where portable or mobile devices are required to exhibit autonomous operation via sophisticated visual processing using on-board hardware resources. The edge applications bring stringent requirements on operating speed (e.g. latency of processing in a fast-flying drone or a head/gaze tracking in a VR system), low-energy (e.g. power consumption in a battery-operated Internet-of-Things sensor device) or data security (e.g. privacy concerns in a security camera). These requirements are best satisfied when the processing of the data stream from an image sensor happens as close as possible to the sensor itself. Even better than attaching a camera to a dedicated microprocessor or mobile GPU is to unify the sensor and processor into a single device. In the paradigm of on-sensor computer vision we perform processing within the sensor itself, on the same chip; and this chip produces abstract, information-rich output rather than images.</p>

<p>It is now well understood that in a computing system most energy and time cost is associated with data communications ‚Äî whether transfers of data between subsystems, or within a subsystem itself (e.g. processor-memory transfers in a conventional CPU or GPU). We have already seen prototypes such as the SCAMP project from the University of Manchester where image sensing, processing and memory are closely integrated with processing carried out by a very large number of processor nodes, each equipped with physically co-located memory. Integration of further processing capabilities on the same silicon chip, possibly within the sensor/processor array itself or in a stacked 3D device may enable a significant fraction of a full computer vision stack to be performed very close to sensing pixels, with an abstract output of only ‚Äúrelevant information‚Äù to other computing elements within a robot or other system. What is considered ‚Äúrelevant information‚Äù depends on the context and application, and a major scientific question to be addressed is how to partition the processing system between the near-sensor computations provided by the pixel-parallel processor array and off-sensor processing hardware.</p>

<h3 id="topics">Topics</h3>
<ul>
  <li>Algorithms for on or near sensor computer vision</li>
  <li>Architectures (digital or analogue)</li>
  <li>Efficiency and power usage</li>
  <li>Cellular automata for vision</li>
  <li>On-sensor neural networks</li>
  <li>Bio-inspired vision sensing</li>
  <li>Graph algorithms for fine-grained parallelism</li>
  <li>Single layer and stacked architectures</li>
  <li>Architecture simulators (functional and hardware level)</li>
  <li>Visualisation and graphics for processor and algorithm design</li>
  <li>Programming and learning for processing arrays</li>
  <li>Analog processing for computer vision</li>
  <li>Dividing processing between on-sensor and off-sensor</li>
  <li>Communication between on-sensor and off-sensor processors (bandwidth and direction)</li>
</ul>
:ET